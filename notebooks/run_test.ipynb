{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fd4259f-a147-4c5a-bde3-7cc9bb13332d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep Learning Models (LOIO-CV Test)\n",
    "- CNN: Convolutional Neural Network\n",
    "- LSTM: Long Short-Term Memory\n",
    "- DCL: DeepConvLSTM\n",
    "- DCLSA: DeepConvLSTMSelfAttention \n",
    "- DCLSA-RN: ResNet version of DCLSA\n",
    "- Transformer\n",
    "- CNN-AE: CNN-based Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05175744-bed7-4fba-bf8a-fb72c0a8ed67",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b1fb1-9a3b-496d-88ef-18978ad61c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\") # Set parent directory to sys.path\n",
    "sys.dont_write_bytecode = True\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src import utils\n",
    "from src.trainer import (\n",
    "    test2, \n",
    "    generate_condition_list,\n",
    "    generate_test_config_target,\n",
    "    generate_results_save_path,\n",
    "    load_and_setup_test_config,\n",
    "    test_setup,\n",
    "    generate_test_results_set,\n",
    "    save_test_results_set\n",
    ")\n",
    "\n",
    "(\n",
    "    plot_parameters, \n",
    "    okabe_ito_color_list, \n",
    "    tol_bright_color_list\n",
    ") = utils.setup_plot(show_color_palette=False)\n",
    "pd.set_option('display.width', 100)\n",
    "\n",
    "confusion_matrix_parameters = {\n",
    "    'font.size': 12,\n",
    "    'axes.labelsize': 12,\n",
    "    'legend.fontsize': 12,\n",
    "    'figure.titlesize': 14,\n",
    "    \"figure.facecolor\": \"white\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de89884-b32d-4b83-a852-168a5c553567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check memory of nvidia cuda in dl-box\n",
    "print(f\"available devices: {torch.cuda.device_count()}\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bdf0c9-1447-41bf-a4ef-08006385188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CUDA_ID = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acbae90-edb0-44d3-bee8-05f5e5363fd8",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630dfb22-3e13-4885-a677-12660a26ee65",
   "metadata": {},
   "source": [
    "## issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa4a31-92e2-48af-b234-be7086eb0cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue = \"I03\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c97800e-a940-4195-bfc0-5ed006f9e857",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3314b56-dec7-434d-81ea-5b7ad9c03adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"om-50\"\n",
    "dataset = \"um-50\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d22c3e-6f05-498f-9d0b-adaf09c16cca",
   "metadata": {},
   "source": [
    "## ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1616b3-0c36-4a16-8e0f-aa42f2088ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex = \"ex-d01\" # flatten-linear or flatten-linear-2\n",
    "# ex = \"ex-d02\" # dropout rate in flatten-linear-2\n",
    "\n",
    "ex = \"ex-d10\" # data augmentation on DCL\n",
    "# ex = \"ex-d11\" # data augmentation on DCL-SA\n",
    "\n",
    "# ex = \"ex-d15\" # mixup argmax or not on DCL\n",
    "# ex = \"ex-d16\" # mixup alpha w/ or w/o random data augmentation on DCL\n",
    "# ex = \"ex-d17\" # mixup alpha after lstm layer of DCL\n",
    "\n",
    "# ex = \"ex-d20\" # unsupervised pretraining of CNN-AE\n",
    "# ex = \"ex-d21\" # no-freeze, soft-freeze, and hard-freeze using pretrained CNN-AE\n",
    "# ex = \"ex-d22\" # CNN-AE w/o\n",
    "\n",
    "# ex = \"ex-d30\" # model comparison\n",
    "\n",
    "# ex = \"ex-d60\" # model hyperparameter tuning of DCLSA\n",
    "# ex = \"ex-d61\" # model hyperparameter tuning of CNN-AE w/o\n",
    "\n",
    "# ex = \"ex-d70\" # data augmentation hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22128cd5-c3d1-466b-a4d3-0914e744846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ex in [\"ex-d01\", \"ex-d02\", \"ex-d10\", \"ex-d15\", \"ex-d16\"]:\n",
    "    model_name = \"dcl\"\n",
    "elif ex in [\"ex-d11\", \"ex-d\"]:\n",
    "    model_name = \"dcl-sa\"\n",
    "elif ex == \"ex-d17\":\n",
    "    model_name = \"dcl-v3\"\n",
    "elif ex == \"ex-d21\":\n",
    "    model_name = \"cnn-ae\"\n",
    "elif ex == \"ex-d22\":\n",
    "    model_name = \"cnn-ae-wo\"\n",
    "elif ex == \"ex-d30\":\n",
    "    model_list = ['cnn', 'lstm', 'dcl', 'dcl-sa', 'resnet-l-sa', 'transformer', 'cnn-ae-wo']\n",
    "    print(f\"model_list: {model_list}\")\n",
    "    model_name = input(\"model_name: \")\n",
    "    while model_name not in model_list:\n",
    "        print(f\"model_name: {model_name} is not in model_list\")\n",
    "        model_name = input(\"model_name: \")\n",
    "elif ex == \"ex-d60\":\n",
    "    model_name = \"dcl-sa\"\n",
    "elif ex == \"ex-d61\":\n",
    "    model_name = \"cnn-ae-wo\"\n",
    "elif ex == \"ex-d70\":\n",
    "    model_name = \"dcl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06d9269-5681-4e7f-ba8e-a70b41b2bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Test config ---\")\n",
    "print(f\"issue: {issue}\")\n",
    "print(f\"ex: {ex}\")\n",
    "print(f\"dataset: {dataset}\")\n",
    "print(f\"model_name: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25bde4b-b932-41d9-a01b-2790c42b4603",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab6155-57f7-42a9-85dd-3b79c186a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_fname = \"best_model_weights.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c5bff1-3430-427e-ba92-36f7c3b115f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_list = generate_condition_list(issue, ex, dataset, model_name)\n",
    "utils.print_path_list_contents_with_index(condition_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e19ac7-5ca4-45b7-8c6d-8b9d1c626c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "# seed_list = [0, 1, 2]\n",
    "seed_list = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79229c0b-1bfe-478f-8d9d-10f95141b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "# check config target\n",
    "for condition in condition_list:\n",
    "    print(f\"condition: {condition}\")\n",
    "    for seed in seed_list:\n",
    "        (\n",
    "            config_target,\n",
    "            path_list\n",
    "        ) = generate_test_config_target(issue, ex, dataset, model_name, condition, seed)\n",
    "        print(f\"config_target: {config_target}\")\n",
    "        config_path_list = sorted(glob.glob(config_target))\n",
    "        print(f\"N of target models: {len(config_path_list)}\") # should be equal to the number of test loio-cv folds\n",
    "        # utils.print_path_list_contents_with_index(config_path_list)\n",
    "        \n",
    "        for config_path in config_path_list:\n",
    "            base_dir = os.path.dirname(config_path)\n",
    "            best_model_path = f\"{base_dir}/checkpoints_dir/best_model_weights.pt\"\n",
    "            if os.path.exists(best_model_path) == True:\n",
    "                continue\n",
    "            else:\n",
    "                print(\"-----------------------\")\n",
    "                print(base_dir)\n",
    "                print(\"-----------------------\")\n",
    "                counter += 1\n",
    "print(\"----------------------------\")\n",
    "print(f\"Error counter: {counter}\")\n",
    "print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a7ef04-4f8c-46b8-8715-6421b9f629b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run test for each condition\n",
    "for c, condition in enumerate(condition_list):\n",
    "    print(\"-------------------------------------------------------------\")\n",
    "    print(f\"condition: {condition}\")\n",
    "    \n",
    "    # run test for each seed\n",
    "    for s, seed in enumerate(seed_list):\n",
    "        print(f\"seed: {seed}\")\n",
    "        \n",
    "        y_gt_all = []\n",
    "        y_pred_all = []\n",
    "        test_animal_id_all = []\n",
    "        \n",
    "        # prepare config path list\n",
    "        config_target, path_list = generate_test_config_target(issue, ex, dataset, model_name, condition, seed)\n",
    "        config_path_list = sorted(glob.glob(config_target))\n",
    "        print(f\"config_target path: {config_target}\")\n",
    "        print(f\"N of target models: {len(config_path_list)}\")\n",
    "        \n",
    "        # results save\n",
    "        results_save_dir, f_basename = generate_results_save_path(path_list, checkpoints_fname)\n",
    "        \n",
    "        for i, config_path in enumerate(config_path_list):\n",
    "            # load test config path\n",
    "            cfg, DEVICE = load_and_setup_test_config(config_path, TEST_CUDA_ID, checkpoints_fname)\n",
    "            \n",
    "            # test setup \n",
    "            test_loader, best_model, optimizer, criterion = test_setup(cfg, config_path, DEVICE)\n",
    "\n",
    "            # Run test\n",
    "            (\n",
    "                y_gt, y_pred, features, _cm, _df_cm, _fig_cm\n",
    "            ) = test2(best_model, optimizer, criterion, test_loader, DEVICE, cfg)\n",
    "            \n",
    "            # array to list\n",
    "            y_gt_list = y_gt.tolist()\n",
    "            y_pred_list = y_pred.tolist()\n",
    "            \n",
    "            # store data for the final calculation\n",
    "            y_gt_all.extend(y_gt_list)\n",
    "            y_pred_all.extend(y_pred_list)\n",
    "            test_animal_id_all.extend([cfg.dataset.test_animal_id]*len(y_gt_list))\n",
    "        \n",
    "        # Results set\n",
    "        plt.rcParams.update(confusion_matrix_parameters)\n",
    "        df_test_scores_all, df_gt_pred_all, fig_cm = generate_test_results_set(\n",
    "            y_gt_all, \n",
    "            y_pred_all, \n",
    "            test_animal_id_all,\n",
    "            cfg\n",
    "        )\n",
    "        \n",
    "        # Save the results set\n",
    "        save_test_results_set(results_save_dir, f_basename, df_test_scores_all, df_gt_pred_all)\n",
    "\n",
    "        print(\"Saved!\")\n",
    "        \n",
    "print(\"-------------------\")\n",
    "print(\"| Test completed. |\")\n",
    "print(\"-------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
